{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# REMEMBER TO CHANGE THE RUNTIME TO GPU!!","metadata":{}},{"cell_type":"code","source":"# !pip install ImageHash","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n# train_df.rename(columns={'multiple_diseases': 'multi'}, inplace=True)\n# to_drop = ['Train_379', 'Train_782', 'Train_1661'] # dropping repeated/mislabeled samples\n# train_df = train_df[~train_df.image_id.isin(to_drop)]\n# test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n# print(train_df.head())\n# print(test_df.head())\n# print(type(train_df.iloc[0]))\n# for iterable in train_df.iloc[0].index:\n#     print(iterable)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_DIR = '/kaggle/input/plant-pathology-2020-fgvc7'\nSEED = 1279","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:04:56.934072Z","iopub.execute_input":"2022-07-15T03:04:56.934340Z","iopub.status.idle":"2022-07-15T03:04:56.938026Z","shell.execute_reply.started":"2022-07-15T03:04:56.934310Z","shell.execute_reply":"2022-07-15T03:04:56.937236Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# image display stuff\n\ndef disp(img_row):\n    img = Image.open(f\"{DATA_DIR}/images/{img_row['image_id']}.jpg\")\n    plt.figure()\n    plt.axis('off')\n    plt.title(', '.join([f'{lbl}={img_row[lbl]}' for lbl in img_row.index]))\n    plt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:52:21.416720Z","iopub.execute_input":"2022-06-28T23:52:21.417035Z","iopub.status.idle":"2022-06-28T23:52:21.423976Z","shell.execute_reply.started":"2022-06-28T23:52:21.417001Z","shell.execute_reply":"2022-06-28T23:52:21.423206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rusty_leaves = train_df[train_df['rust'] > 0]\nidxs = np.random.choice(len(rusty_leaves), size=10)\nfor idx in idxs:\n    disp(rusty_leaves.iloc[idx])","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:52:21.622198Z","iopub.execute_input":"2022-06-28T23:52:21.622499Z","iopub.status.idle":"2022-06-28T23:52:21.721304Z","shell.execute_reply.started":"2022-06-28T23:52:21.622460Z","shell.execute_reply":"2022-06-28T23:52:21.719056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scabby = train_df[train_df['scab'] > 0]\n# idxs = np.random.choice(len(scabby), size=10)\n# for idx in idxs:\n#     disp(scabby.iloc[idx])","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:52:21.842464Z","iopub.execute_input":"2022-06-28T23:52:21.843487Z","iopub.status.idle":"2022-06-28T23:52:21.849170Z","shell.execute_reply.started":"2022-06-28T23:52:21.843430Z","shell.execute_reply":"2022-06-28T23:52:21.848383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helth = train_df[train_df['healthy'] > 0]\n# idxs = np.random.choice(len(helth), size=10)\n# for idx in idxs:\n#     disp(helth.iloc[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idxs = np.random.choice(len(test_df), size=10)\n# for i in idxs:\n#     disp(test_df.iloc[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # looking for similar images (https://github.com/JohannesBuchner/imagehash/blob/master/find_similar_images.py)\n# import imagehash\n# def find_similar_images(userpaths, hashfunc=imagehash.phash):\n#     def is_image(filename):\n#         f = filename.lower()\n#         return f.endswith(\".png\") or f.endswith(\".jpg\") or \\\n#             f.endswith(\".jpeg\") or f.endswith(\".bmp\") or \\\n#             f.endswith(\".gif\") or '.jpg' in f or  f.endswith(\".svg\")\n    \n#     image_filenames = []\n#     for userpath in userpaths:\n#         image_filenames += [os.path.join(userpath, path) for path in os.listdir(userpath) if is_image(path)]\n#     images = {}\n#     problematic = []\n#     print(\"filenames obtained\")\n#     for img in tqdm(sorted(image_filenames)):\n#         try:\n#             hash = hashfunc(Image.open(img))\n#         except Exception as e:\n#             print('Problem:', e, 'with', img)\n#             continue\n#         if hash in images:\n#             print(img, '  already exists as', ' '.join(images[hash]))\n#             problematic += [img, *images[hash]]\n# #             if 'dupPictures' in img:\n# #                 print('rm -v', img)\n#         images[hash] = images.get(hash, []) + [img]\n#     return problematic\n    \n# problematic = find_similar_images([f'{DATA_DIR}/images/'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # print(problematic)\n# pairs = []\n# for k in range(0,len(problematic),2):\n#     pairs.append( (problematic[k].rsplit('/', 1)[-1], problematic[k+1].rsplit('/', 1)[-1]) )\n# print(pairs[-3:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# idxs = [379, 1173, 782, 592, 815, 1661]\n# for i in idxs:\n#     disp(train_df.iloc[i])\n    \n# # seems like we should drop Train_379, Train_782, and Train_1661","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for col in ['healthy', 'multi', 'rust', 'scab']:\n#     print(f\"{col}, {sum(train_df[col])}\")\n    \n# # data distribution: fairly even except for less multi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\nNote: This is designed to be runnable even if the data analysis part is not run. As such, there's some code repeat.","metadata":{}},{"cell_type":"code","source":"from torch.utils.data.dataset import Dataset, Subset\nfrom torch.utils.data import DataLoader\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom sklearn.metrics import roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:04.583841Z","iopub.execute_input":"2022-07-15T03:05:04.584102Z","iopub.status.idle":"2022-07-15T03:05:12.233071Z","shell.execute_reply.started":"2022-07-15T03:05:04.584072Z","shell.execute_reply":"2022-07-15T03:05:12.232286Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Utility Functions","metadata":{}},{"cell_type":"code","source":"def get_train_data():\n    train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\n    train_df.rename(columns={'multiple_diseases': 'multi'}, inplace=True)\n    to_drop = ['Train_379', 'Train_782', 'Train_1661'] # dropping repeated/mislabeled samples\n    train_df = train_df[~train_df.image_id.isin(to_drop)]\n    files = [f\"{DATA_DIR}/images/{fname}.jpg\" for fname in train_df['image_id']]\n    labels = train_df.iloc[:, 1:].to_numpy().argmax(axis=1).astype(int)\n    return files, labels","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:12.234818Z","iopub.execute_input":"2022-07-15T03:05:12.235260Z","iopub.status.idle":"2022-07-15T03:05:12.244303Z","shell.execute_reply.started":"2022-07-15T03:05:12.235211Z","shell.execute_reply":"2022-07-15T03:05:12.243576Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def get_test_data():\n    test_df = pd.read_csv(f'{DATA_DIR}/test.csv')\n    files = [f\"{DATA_DIR}/images/{fname}.jpg\" for fname in test_df['image_id']]\n    return files, None","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:12.245315Z","iopub.execute_input":"2022-07-15T03:05:12.246574Z","iopub.status.idle":"2022-07-15T03:05:12.253396Z","shell.execute_reply.started":"2022-07-15T03:05:12.246525Z","shell.execute_reply":"2022-07-15T03:05:12.252476Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def get_transforms(train=True):\n    transforms = torchvision.transforms.Compose([\n            torchvision.transforms.Resize((224, 224)), # change this to some variable later!\n            torchvision.transforms.ToTensor()\n    ])\n    return transforms","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:12.254847Z","iopub.execute_input":"2022-07-15T03:05:12.258247Z","iopub.status.idle":"2022-07-15T03:05:12.264734Z","shell.execute_reply.started":"2022-07-15T03:05:12.258206Z","shell.execute_reply":"2022-07-15T03:05:12.263979Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def compute_acc(outputs, labels):\n    return np.mean(outputs==labels)\n\ndef compute_mean_auc(outputs, labels):\n    return roc_auc_score(labels, outputs, average='macro', multi_class='ovr')","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:12.265993Z","iopub.execute_input":"2022-07-15T03:05:12.266664Z","iopub.status.idle":"2022-07-15T03:05:12.279849Z","shell.execute_reply.started":"2022-07-15T03:05:12.266628Z","shell.execute_reply":"2022-07-15T03:05:12.279073Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Dataset and Dataloader setup","metadata":{}},{"cell_type":"code","source":"class AppleDataset(Dataset):\n    def __init__(self, filepaths, transforms, labels=None):\n        self.transforms = transforms\n        self.filepaths = filepaths\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.filepaths)\n    \n    def __getitem__(self, i):\n        img = Image.open(self.filepaths[i]).convert('RGB')\n        img = self.transforms(img)\n        lbl = self.labels[i] if self.labels is not None else None\n        return (img, lbl)","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:12.791294Z","iopub.execute_input":"2022-07-15T03:05:12.791957Z","iopub.status.idle":"2022-07-15T03:05:12.799519Z","shell.execute_reply.started":"2022-07-15T03:05:12.791912Z","shell.execute_reply":"2022-07-15T03:05:12.798646Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Lightning folds !\nclass AppleKFoldDataModule(pl.LightningDataModule):\n    def __init__(self, k_folds=5):\n        self.k_folds = k_folds\n        super().__init__()\n    \n    def setup(self, stage=None):\n        transforms = get_transforms()\n        train_files, train_labels = get_train_data()\n        test_files = get_test_data()\n        self.train_dset = AppleDataset(train_files, transforms, labels=train_labels)\n        self.test_dset = AppleDataset(test_files, transforms)\n    \n    def setup_folds(self):\n        rng = np.random.default_rng(seed=SEED)\n        shuffled_idx = rng.permutation(len(self.train_dset))\n        self.splits = np.array_split(shuffled_idx, self.k_folds)\n        \n    def setup_fold_index(self, i):\n        train_idx = np.concatenate(self.splits[:i] + self.splits[i+1:])\n        val_idx = self.splits[i]\n        self.train_fold = Subset(self.train_dset, train_idx)\n        self.val_fold = Subset(self.train_dset, val_idx)\n    \n    def train_dataloader(self, bsz=32):\n        return DataLoader(self.train_fold, batch_size=bsz, shuffle=True) # change batch size for variable\n    \n    def val_dataloader(self, bsz=32):\n        return DataLoader(self.val_fold, batch_size=bsz, shuffle=False)\n        \n    def test_dataloader(self, bsz=32):\n        return DataLoader(self.test_dset, batch_size=bsz, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:13.118229Z","iopub.execute_input":"2022-07-15T03:05:13.118867Z","iopub.status.idle":"2022-07-15T03:05:13.137473Z","shell.execute_reply.started":"2022-07-15T03:05:13.118823Z","shell.execute_reply":"2022-07-15T03:05:13.136664Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Model(s) Setup","metadata":{}},{"cell_type":"code","source":"def initialize_base_model(model_name, pretrain=True):\n    if model_name == 'resnet-50':\n        model = torchvision.models.resnet50(pretrained=pretrain)\n        inp_size = 224\n        out_size = 1000\n        \n    return model, inp_size, out_size","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:13.764366Z","iopub.execute_input":"2022-07-15T03:05:13.765317Z","iopub.status.idle":"2022-07-15T03:05:13.770480Z","shell.execute_reply.started":"2022-07-15T03:05:13.765276Z","shell.execute_reply":"2022-07-15T03:05:13.769601Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class GenericAppleArch(nn.Module):\n    def __init__(self, base_model_name='resnet-50', out_classes=4):\n        super(GenericAppleArch, self).__init__()\n        self.ft_extractor, self.inp_size, out_size = initialize_base_model(base_model_name, pretrain=True)\n        self.clfr = nn.Sequential(nn.Linear(out_size, out_classes), nn.Softmax(dim=1))\n        \n    def forward(self, x):\n        x = self.ft_extractor(x)\n        out = self.clfr(x)\n        return out\n    \n\nclass AppleModel(pl.LightningModule): #Lightning-fied\n    def __init__(self, base_model_name='resnet-50', out_classes=4, lr=1e-5):\n        super().__init__()\n        self.model = GenericAppleArch(base_model_name=base_model_name, out_classes=out_classes)\n        self.lr = lr\n        self.save_hyperparameters()\n        self.xvals = []\n        \n    def on_train_epoch_start(self):\n        self.train_outputs = []\n        self.train_ys = []\n    \n    def on_validation_epoch_start(self):\n        self.val_outputs = []\n        self.val_ys = []\n        \n    def training_step(self, batch, batch_idx):\n        x,y = batch\n        out = self.model(x)\n        loss = F.nll_loss(out, y)\n        self.train_outputs += list(out.detach().cpu().numpy())\n        self.train_ys += list(y.detach().cpu())\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x,y = batch\n        out = self.model(x)\n        self.val_outputs += list(out.detach().cpu().numpy())\n        self.val_ys += list(y.detach().cpu().numpy())\n        loss = F.nll_loss(out, y)\n        self.log('val_loss', loss)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        return optimizer\n    \n    def on_train_epoch_end(self):\n        all_outputs = np.array(self.train_outputs)\n        all_preds = np.argmax(all_outputs, axis=1)\n        all_ys = np.array(self.train_ys)\n        epoch_acc = compute_acc(all_preds, all_ys)\n        mean_auc = compute_mean_auc(all_outputs, all_ys)\n        self.log_dict({'train_acc': epoch_acc, 'train_mean_auc': mean_auc})\n        print(f\"Train acc={epoch_acc}\")\n        print(f\"Train mean auc={mean_auc}\")\n    \n    def on_validation_epoch_end(self):\n        all_outputs = np.array(self.val_outputs)\n        all_preds = np.argmax(all_outputs, axis=1)\n        all_ys = np.array(self.val_ys)\n        epoch_acc = compute_acc(all_preds, all_ys)\n        mean_auc = compute_mean_auc(all_outputs, all_ys)\n        self.log_dict({'val_acc': epoch_acc, 'val_mean_auc': mean_auc})\n        print(f\"Val acc={epoch_acc}\")\n        print(f\"Val mean auc={mean_auc}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:14.245125Z","iopub.execute_input":"2022-07-15T03:05:14.245651Z","iopub.status.idle":"2022-07-15T03:05:14.263739Z","shell.execute_reply.started":"2022-07-15T03:05:14.245616Z","shell.execute_reply":"2022-07-15T03:05:14.263030Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Training Loop","metadata":{}},{"cell_type":"code","source":"from copy import deepcopy\nfrom datetime import datetime\nfrom pytz import timezone\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:15.188231Z","iopub.execute_input":"2022-07-15T03:05:15.188884Z","iopub.status.idle":"2022-07-15T03:05:15.193391Z","shell.execute_reply.started":"2022-07-15T03:05:15.188845Z","shell.execute_reply":"2022-07-15T03:05:15.192701Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"## WANDB\n# https://wandb.ai/manan-goel/MNIST/reports/How-to-Integrate-PyTorch-Lightning-with-Weights-Biases--VmlldzoxNjg1ODQ1\nimport wandb\nfrom pytorch_lightning.loggers import WandbLogger\nfrom kaggle_secrets import UserSecretsClient\nimport os\nuser_secrets = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"wandbKey\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pl.seed_everything(SEED)\nexp_name = \"baseline\"\ntry:\n    wandb.finish()\nexcept:\n    pass\nk_models = []\nconfig = {\n    'trainer_kwargs': \n        {\n            'max_epochs': 30,\n            'accelerator': \"gpu\" if torch.cuda.is_available() else \"cpu\",\n            'default_root_dir': \"/kaggle/working/\",\n            'auto_lr_find': False,\n            'log_every_n_steps': 10,\n        },\n    'model_kwargs':\n        {\n            'base_model_name': 'resnet-50', \n            'out_classes': 4,\n            'lr': 1e-5\n        }\n}\ndm = AppleKFoldDataModule(k_folds=5)\ndm.prepare_data()\ndm.setup()\ndm.setup_folds()\nmodel = AppleModel()\nbase_state_dict = model.state_dict()\nfor k in range(dm.k_folds):\n    time = datetime.now(timezone('US/Pacific')).strftime(\"%m_%d-h%H_m%M\")\n    save_filepath = f\"{exp_name}-fold{k+1}of{dm.k_folds}-{time}\"\n    config['trainer_kwargs']['logger'] = WandbLogger(project=\"apple-kaggle\", entity=\"waggle\", name=f\"{exp_name}_fold{k+1}of{dm.k_folds}\")\n    config['trainer_kwargs']['callbacks'] = [\n        EarlyStopping(monitor=\"val_loss\", mode=\"min\"),\n        checkpoint_callback = ModelCheckpoint(dirpath=\"/kaggle/working/\", save_top_k=1, monitor=\"val_loss\", filename=save_filepath)\n    ]\n    trainer = pl.Trainer(**config['trainer_kwargs'])\n    dm.setup_fold_index(k)\n    model.load_state_dict(base_state_dict)\n    trainer.fit(model, datamodule=dm)\n    wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Analysis\nTesting results of completed models","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:30.945515Z","iopub.execute_input":"2022-07-15T03:05:30.946120Z","iopub.status.idle":"2022-07-15T03:05:30.950399Z","shell.execute_reply.started":"2022-07-15T03:05:30.946080Z","shell.execute_reply":"2022-07-15T03:05:30.949617Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# input_model_name = \"baseline-fold4of5-06_18-h10_m19.ckpt\"\niput_model_name = '../input/v65apple/baseline-fold4of5-06_18-h10_m19.ckpt'\n# model_path = f\"/kaggle/input/appleanalysis/{input_model_name}\"\nmodel_path = \"/kaggle/input/v65apple/baseline-fold4of5-06_18-h10_m19.ckpt\"\nckpt = torch.load(model_path)\n# old_model = AppleModel(**ckpt['hyper_parameters'])\nold_model = AppleModel.load_from_checkpoint(model_path)","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:05:33.350124Z","iopub.execute_input":"2022-07-15T03:05:33.350380Z","iopub.status.idle":"2022-07-15T03:05:46.573954Z","shell.execute_reply.started":"2022-07-15T03:05:33.350352Z","shell.execute_reply":"2022-07-15T03:05:46.573214Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/97.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25c89f2d33d346ffa6ce234dc17d725b"}},"metadata":{}}]},{"cell_type":"code","source":"\n\npl.seed_everything(SEED)\ndm = AppleKFoldDataModule(k_folds=5)\ndm.prepare_data()\ndm.setup()\ndm.setup_folds()\ndm.setup_fold_index(3)\ndl = dm.val_dataloader()\n\n# comment out below if you only want to get the model\nold_model.eval()\nold_model.on_validation_epoch_start()\nfor i, (x,y) in enumerate(tqdm(dl)):\n    old_model.validation_step((x,y), i)\n    \n    \n\noutputs = np.array(old_model.val_outputs)\nlabels = old_model.val_ys\nprint(outputs[0])\nprint(labels[0])","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:42:13.059057Z","iopub.execute_input":"2022-07-15T03:42:13.059579Z","iopub.status.idle":"2022-07-15T03:43:21.085056Z","shell.execute_reply.started":"2022-07-15T03:42:13.059524Z","shell.execute_reply":"2022-07-15T03:43:21.084305Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"  0%|          | 0/12 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py:373: UserWarning: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n  \"You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet.\"\n100%|██████████| 12/12 [01:07<00:00,  5.67s/it]","output_type":"stream"},{"name":"stdout","text":"[1.6899587e-03 6.3926098e-05 1.2241739e-04 9.9812371e-01]\n3\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_csv(f'{DATA_DIR}/train.csv')\ntrain_df.rename(columns={'multiple_diseases': 'multi'}, inplace=True)\nto_drop = ['Train_379', 'Train_782', 'Train_1661'] # dropping repeated/mislabeled samples\ntrain_df = train_df[~train_df.image_id.isin(to_drop)]","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:56:10.438630Z","iopub.execute_input":"2022-06-28T23:56:10.439120Z","iopub.status.idle":"2022-06-28T23:56:10.451234Z","shell.execute_reply.started":"2022-06-28T23:56:10.439083Z","shell.execute_reply":"2022-06-28T23:56:10.449687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = train_df.iloc[dm.val_fold.indices]\n\nval_outputs = np.array(old_model.val_outputs)\nval_df.loc[:,'pred_healthy'] = outputs[:,0].round(5)\nval_df.loc[:,'pred_multi'] = outputs[:,1].round(5)\nval_df.loc[:,'pred_rust'] = outputs[:,2].round(5)\nval_df.loc[:,'pred_scab'] = outputs[:,3].round(5)\nval_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:56:55.762294Z","iopub.execute_input":"2022-06-28T23:56:55.762600Z","iopub.status.idle":"2022-06-28T23:56:55.788602Z","shell.execute_reply.started":"2022-06-28T23:56:55.762568Z","shell.execute_reply":"2022-06-28T23:56:55.787878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 0\nerror = 0\nwhile i < len(labels) and error < 20:\n    if labels[i] != np.argmax(outputs[i]):\n        disp(val_df.iloc[i])\n        error += 1\n        # :)\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:57:16.331747Z","iopub.execute_input":"2022-06-28T23:57:16.332424Z","iopub.status.idle":"2022-06-28T23:57:26.150174Z","shell.execute_reply.started":"2022-06-28T23:57:16.332390Z","shell.execute_reply":"2022-06-28T23:57:26.149499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(np.argmax(outputs[0]))\nprint(outputs[0])\nprint(labels[0])\nprint(xvals[0].shape)\nplt.imshow(xvals[0].transpose(1,2,0))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:16:38.153863Z","iopub.execute_input":"2022-06-28T23:16:38.154147Z","iopub.status.idle":"2022-06-28T23:16:38.396203Z","shell.execute_reply.started":"2022-06-28T23:16:38.154116Z","shell.execute_reply":"2022-06-28T23:16:38.395393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all errors\nerrors = []\nfor i in range(len(labels)):\n    if labels[i] != np.argmax(outputs[i]):\n        errors.append((outputs[i], labels[i], xvals[i].transpose(1,2,0)))","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:27:42.963214Z","iopub.execute_input":"2022-06-28T23:27:42.963770Z","iopub.status.idle":"2022-06-28T23:27:42.971099Z","shell.execute_reply.started":"2022-06-28T23:27:42.963731Z","shell.execute_reply":"2022-06-28T23:27:42.970138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(errors)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T23:27:54.237565Z","iopub.execute_input":"2022-06-28T23:27:54.238210Z","iopub.status.idle":"2022-06-28T23:27:54.248775Z","shell.execute_reply.started":"2022-06-28T23:27:54.238163Z","shell.execute_reply":"2022-06-28T23:27:54.247327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def disp(img_row):\n    img = Image.open(f\"{DATA_DIR}/images/{img_row['image_id']}.jpg\")\n    plt.figure()\n    plt.axis('off')\n    plt.title(', '.join([f'{lbl}={img_row[lbl]}' for lbl in img_row.index]))\n    plt.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_error(ind):\n    out, label, img = errors[ind]\n    print(out)\n    print(label)\n    plt.imshow(img)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(errors)):\n    display_error(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GRADCAM ANALYSIS","metadata":{}},{"cell_type":"code","source":"import sys\n!pip install ttach\n# !git clone https://github.com/jacobgil/pytorch-grad-cam\nsys.path.append(\"../input/gradcam\")\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:27:26.797121Z","iopub.execute_input":"2022-07-15T03:27:26.797376Z","iopub.status.idle":"2022-07-15T03:27:38.221358Z","shell.execute_reply.started":"2022-07-15T03:27:26.797347Z","shell.execute_reply":"2022-07-15T03:27:38.220585Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Collecting ttach\n  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\nInstalling collected packages: ttach\nSuccessfully installed ttach-0.0.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# get an image from a batch for input tensor\ninputs, classes = next(iter(dl)) \ninputs","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:35:50.294924Z","iopub.execute_input":"2022-07-15T03:35:50.295186Z","iopub.status.idle":"2022-07-15T03:35:52.155351Z","shell.execute_reply.started":"2022-07-15T03:35:50.295157Z","shell.execute_reply":"2022-07-15T03:35:52.154577Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([[[[0.1098, 0.1098, 0.1137,  ..., 0.3804, 0.3686, 0.3647],\n          [0.1059, 0.1098, 0.1176,  ..., 0.3882, 0.3765, 0.3647],\n          [0.1059, 0.1098, 0.1176,  ..., 0.3922, 0.3765, 0.3608],\n          ...,\n          [0.8118, 0.8039, 0.7961,  ..., 0.1098, 0.1098, 0.1098],\n          [0.8157, 0.8039, 0.8000,  ..., 0.1137, 0.1098, 0.1059],\n          [0.8157, 0.8078, 0.8039,  ..., 0.1098, 0.1098, 0.1059]],\n\n         [[0.2275, 0.2314, 0.2392,  ..., 0.4941, 0.4863, 0.4824],\n          [0.2235, 0.2314, 0.2392,  ..., 0.4902, 0.4784, 0.4706],\n          [0.2275, 0.2314, 0.2392,  ..., 0.4863, 0.4706, 0.4588],\n          ...,\n          [0.8627, 0.8667, 0.8627,  ..., 0.2157, 0.2157, 0.2157],\n          [0.8627, 0.8667, 0.8667,  ..., 0.2196, 0.2157, 0.2118],\n          [0.8627, 0.8627, 0.8627,  ..., 0.2196, 0.2157, 0.2157]],\n\n         [[0.0863, 0.0824, 0.0784,  ..., 0.3686, 0.3529, 0.3451],\n          [0.0824, 0.0824, 0.0824,  ..., 0.3765, 0.3608, 0.3490],\n          [0.0784, 0.0784, 0.0824,  ..., 0.3804, 0.3647, 0.3490],\n          ...,\n          [0.7373, 0.7373, 0.7373,  ..., 0.0784, 0.0784, 0.0784],\n          [0.7490, 0.7451, 0.7412,  ..., 0.0824, 0.0784, 0.0745],\n          [0.7451, 0.7451, 0.7451,  ..., 0.0784, 0.0784, 0.0784]]],\n\n\n        [[[0.2471, 0.2471, 0.2510,  ..., 0.4706, 0.4471, 0.4157],\n          [0.2510, 0.2510, 0.2549,  ..., 0.4784, 0.4510, 0.4196],\n          [0.2510, 0.2549, 0.2549,  ..., 0.4863, 0.4627, 0.4353],\n          ...,\n          [0.0784, 0.0902, 0.0902,  ..., 0.5882, 0.5451, 0.5059],\n          [0.0863, 0.0902, 0.0941,  ..., 0.5569, 0.5137, 0.4784],\n          [0.0941, 0.1059, 0.1137,  ..., 0.5373, 0.5098, 0.4745]],\n\n         [[0.2980, 0.2980, 0.3020,  ..., 0.5216, 0.4980, 0.4706],\n          [0.3020, 0.3020, 0.3059,  ..., 0.5294, 0.5059, 0.4784],\n          [0.3020, 0.3059, 0.3098,  ..., 0.5412, 0.5176, 0.4902],\n          ...,\n          [0.1255, 0.1373, 0.1412,  ..., 0.4902, 0.4510, 0.4157],\n          [0.1333, 0.1373, 0.1451,  ..., 0.4667, 0.4314, 0.4000],\n          [0.1333, 0.1490, 0.1765,  ..., 0.4510, 0.4275, 0.4039]],\n\n         [[0.1176, 0.1176, 0.1098,  ..., 0.2510, 0.2510, 0.2353],\n          [0.1216, 0.1216, 0.1137,  ..., 0.2588, 0.2471, 0.2314],\n          [0.1216, 0.1255, 0.1176,  ..., 0.2667, 0.2510, 0.2392],\n          ...,\n          [0.0314, 0.0392, 0.0431,  ..., 0.5020, 0.4549, 0.4157],\n          [0.0392, 0.0431, 0.0431,  ..., 0.4627, 0.4275, 0.3922],\n          [0.0353, 0.0510, 0.0667,  ..., 0.4314, 0.4078, 0.3804]]],\n\n\n        [[[0.6353, 0.7176, 0.8980,  ..., 0.3373, 0.3490, 0.3569],\n          [0.6588, 0.7647, 0.9333,  ..., 0.3333, 0.3412, 0.3529],\n          [0.7059, 0.8392, 0.9725,  ..., 0.3255, 0.3373, 0.3451],\n          ...,\n          [0.6118, 0.5216, 0.4902,  ..., 0.4627, 0.4510, 0.4353],\n          [0.6980, 0.5725, 0.5098,  ..., 0.4667, 0.4627, 0.4549],\n          [0.7686, 0.6353, 0.5490,  ..., 0.4745, 0.4706, 0.4627]],\n\n         [[0.6471, 0.7294, 0.8980,  ..., 0.4118, 0.4235, 0.4314],\n          [0.6667, 0.7686, 0.9373,  ..., 0.4078, 0.4157, 0.4275],\n          [0.7137, 0.8392, 0.9725,  ..., 0.4000, 0.4118, 0.4196],\n          ...,\n          [0.6549, 0.5765, 0.5412,  ..., 0.5020, 0.5020, 0.4902],\n          [0.7176, 0.6196, 0.5569,  ..., 0.5059, 0.4980, 0.4902],\n          [0.7804, 0.6706, 0.5843,  ..., 0.5059, 0.4941, 0.4941]],\n\n         [[0.5686, 0.7020, 0.9451,  ..., 0.2353, 0.2471, 0.2549],\n          [0.6039, 0.7725, 0.9725,  ..., 0.2314, 0.2392, 0.2510],\n          [0.6745, 0.8745, 0.9922,  ..., 0.2314, 0.2392, 0.2471],\n          ...,\n          [0.6627, 0.5490, 0.5098,  ..., 0.2902, 0.2902, 0.2902],\n          [0.7725, 0.6275, 0.5412,  ..., 0.3059, 0.3137, 0.3216],\n          [0.8549, 0.7176, 0.5882,  ..., 0.3255, 0.3412, 0.3451]]],\n\n\n        ...,\n\n\n        [[[0.1804, 0.1843, 0.1843,  ..., 0.5451, 0.4588, 0.5137],\n          [0.1882, 0.1922, 0.1922,  ..., 0.6588, 0.5647, 0.5490],\n          [0.2039, 0.2039, 0.2000,  ..., 0.7176, 0.6824, 0.6431],\n          ...,\n          [0.2196, 0.2157, 0.2039,  ..., 0.5451, 0.5333, 0.5098],\n          [0.2235, 0.2196, 0.2039,  ..., 0.5529, 0.5373, 0.5098],\n          [0.2275, 0.2196, 0.2078,  ..., 0.5569, 0.5412, 0.5216]],\n\n         [[0.3176, 0.3176, 0.3176,  ..., 0.6745, 0.6314, 0.6824],\n          [0.3255, 0.3255, 0.3255,  ..., 0.7373, 0.6980, 0.7059],\n          [0.3412, 0.3373, 0.3333,  ..., 0.7490, 0.7529, 0.7412],\n          ...,\n          [0.3686, 0.3686, 0.3608,  ..., 0.8549, 0.8392, 0.8157],\n          [0.3725, 0.3725, 0.3608,  ..., 0.8627, 0.8431, 0.8157],\n          [0.3765, 0.3686, 0.3647,  ..., 0.8627, 0.8471, 0.8314]],\n\n         [[0.2392, 0.2431, 0.2510,  ..., 0.4078, 0.3451, 0.4275],\n          [0.2471, 0.2510, 0.2588,  ..., 0.5098, 0.4392, 0.4431],\n          [0.2627, 0.2627, 0.2667,  ..., 0.5765, 0.5373, 0.4941],\n          ...,\n          [0.2235, 0.2275, 0.2235,  ..., 0.2000, 0.1922, 0.1725],\n          [0.2275, 0.2275, 0.2235,  ..., 0.2078, 0.1961, 0.1725],\n          [0.2314, 0.2275, 0.2275,  ..., 0.2118, 0.1961, 0.1765]]],\n\n\n        [[[0.2118, 0.2118, 0.2078,  ..., 0.7647, 0.7529, 0.7451],\n          [0.2118, 0.2157, 0.2118,  ..., 0.7608, 0.7529, 0.7451],\n          [0.2157, 0.2157, 0.2157,  ..., 0.7569, 0.7490, 0.7412],\n          ...,\n          [0.4588, 0.3843, 0.1961,  ..., 0.5961, 0.5843, 0.5804],\n          [0.4824, 0.4275, 0.2510,  ..., 0.6000, 0.5843, 0.5804],\n          [0.4980, 0.3882, 0.2471,  ..., 0.5922, 0.5882, 0.5843]],\n\n         [[0.3725, 0.3765, 0.3765,  ..., 0.8824, 0.8706, 0.8627],\n          [0.3725, 0.3765, 0.3804,  ..., 0.8784, 0.8706, 0.8627],\n          [0.3725, 0.3765, 0.3843,  ..., 0.8745, 0.8667, 0.8588],\n          ...,\n          [0.5333, 0.4863, 0.3490,  ..., 0.6000, 0.5765, 0.5725],\n          [0.5451, 0.5137, 0.3922,  ..., 0.6000, 0.5804, 0.5725],\n          [0.5490, 0.4784, 0.3843,  ..., 0.6000, 0.5843, 0.5725]],\n\n         [[0.3137, 0.3020, 0.3020,  ..., 0.6078, 0.5922, 0.5804],\n          [0.3137, 0.3059, 0.3059,  ..., 0.6039, 0.5922, 0.5804],\n          [0.3176, 0.3098, 0.3098,  ..., 0.6000, 0.5843, 0.5765],\n          ...,\n          [0.4824, 0.3569, 0.1922,  ..., 0.5569, 0.5294, 0.5255],\n          [0.4941, 0.3882, 0.2275,  ..., 0.5490, 0.5294, 0.5255],\n          [0.5059, 0.3765, 0.2353,  ..., 0.5412, 0.5333, 0.5255]]],\n\n\n        [[[0.9059, 0.9176, 0.9294,  ..., 0.0941, 0.0863, 0.0863],\n          [0.9294, 0.9176, 0.8980,  ..., 0.0902, 0.0863, 0.0863],\n          [0.9020, 0.8784, 0.8118,  ..., 0.0863, 0.0863, 0.0863],\n          ...,\n          [0.1686, 0.1451, 0.1333,  ..., 0.4157, 0.4314, 0.4078],\n          [0.1569, 0.1490, 0.1529,  ..., 0.4196, 0.4392, 0.4039],\n          [0.1490, 0.1451, 0.1333,  ..., 0.4078, 0.4275, 0.4078]],\n\n         [[0.9137, 0.9255, 0.9490,  ..., 0.1922, 0.1804, 0.1804],\n          [0.9373, 0.9333, 0.9294,  ..., 0.1843, 0.1804, 0.1804],\n          [0.9059, 0.8980, 0.8627,  ..., 0.1804, 0.1804, 0.1765],\n          ...,\n          [0.3020, 0.2745, 0.2549,  ..., 0.4902, 0.5255, 0.5412],\n          [0.2902, 0.2784, 0.2745,  ..., 0.4941, 0.5255, 0.5490],\n          [0.2824, 0.2784, 0.2588,  ..., 0.4902, 0.5059, 0.5451]],\n\n         [[0.8941, 0.9098, 0.9333,  ..., 0.0824, 0.0784, 0.0784],\n          [0.9176, 0.9137, 0.9098,  ..., 0.0863, 0.0824, 0.0784],\n          [0.8902, 0.8824, 0.8275,  ..., 0.0863, 0.0784, 0.0745],\n          ...,\n          [0.3098, 0.2824, 0.2588,  ..., 0.3216, 0.4000, 0.4235],\n          [0.2980, 0.2863, 0.2784,  ..., 0.3255, 0.3922, 0.4157],\n          [0.2863, 0.2824, 0.2510,  ..., 0.3176, 0.3686, 0.4196]]]])"},"metadata":{}}]},{"cell_type":"code","source":"\n\ntarget_layers = [old_model.model.ft_extractor.layer4[-1]]\ninput_tensor = inputs# Create an input tensor image for your model..\n# Note: input_tensor can be a batch tensor with several images!\n\n# Construct the CAM object once, and then re-use it on many images:\ncam = GradCAM(model=old_model, target_layers=target_layers, use_cuda=False) #args.use_cuda)\n\n# You can also use it within a with statement, to make sure it is freed,\n# In case you need to re-create it inside an outer loop:\n# with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam:\n#   ...\n\n# We have to specify the target we want to generate\n# the Class Activation Maps for.\n# If targets is None, the highest scoring category\n# will be used for every image in the batch.\n# Here we use ClassifierOutputTarget, but you can define your own custom targets\n# That are, for example, combinations of categories, or specific outputs in a non standard model.\n\ntargets = [ClassifierOutputTarget(281)]\n\n# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\ngrayscale_cam = cam(input_tensor=input_tensor, targets=classes)\n\n# In this example grayscale_cam has only one image in the batch:\ngrayscale_cam = grayscale_cam[0, :]\nvisualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:39:17.000767Z","iopub.execute_input":"2022-07-15T03:39:17.001464Z","iopub.status.idle":"2022-07-15T03:39:17.037465Z","shell.execute_reply.started":"2022-07-15T03:39:17.001414Z","shell.execute_reply":"2022-07-15T03:39:17.036389Z"},"trusted":true},"execution_count":31,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/1917631997.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgrayscale_cam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# In this example grayscale_cam has only one image in the batch:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/gradcam/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         return self.forward(input_tensor,\n\u001b[0;32m--> 185\u001b[0;31m                             targets, eigen_smooth)\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/gradcam/pytorch_grad_cam/base_cam.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                                    requires_grad=True)\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mtarget_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/gradcam/pytorch_grad_cam/activations_and_gradients.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0mYour\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: "],"ename":"NotImplementedError","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"old_model.model.ft_extractor.layer4","metadata":{"execution":{"iopub.status.busy":"2022-07-15T03:38:54.528331Z","iopub.execute_input":"2022-07-15T03:38:54.528599Z","iopub.status.idle":"2022-07-15T03:38:54.534631Z","shell.execute_reply.started":"2022-07-15T03:38:54.528567Z","shell.execute_reply":"2022-07-15T03:38:54.533851Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Sequential(\n  (0): Bottleneck(\n    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (1): Bottleneck(\n    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n  (2): Bottleneck(\n    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}